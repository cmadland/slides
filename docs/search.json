[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/otessa25-scale/index.html#developing-the-technology-integrated-assessment-scale",
    "href": "posts/otessa25-scale/index.html#developing-the-technology-integrated-assessment-scale",
    "title": "OTESSA25",
    "section": "Developing the Technology-integrated Assessment Scale",
    "text": "Developing the Technology-integrated Assessment Scale"
  },
  {
    "objectID": "posts/otessa25-scale/index.html#acknowledgements",
    "href": "posts/otessa25-scale/index.html#acknowledgements",
    "title": "OTESSA25",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nOur team is spread across most of the country and we each live and work on land that has been cared for by Indigenous people for millenia. We each acknowledge that our abundant lives were made possible because of the displacement of the original stewards of the land."
  },
  {
    "objectID": "posts/otessa25-scale/index.html#otessa24",
    "href": "posts/otessa25-scale/index.html#otessa24",
    "title": "OTESSA25",
    "section": "OTESSA24",
    "text": "OTESSA24\nEvolving our understanding of technology-integrated assessment: A review of the literature and development of a new framework"
  },
  {
    "objectID": "posts/otessa25-scale/index.html#two-papers",
    "href": "posts/otessa25-scale/index.html#two-papers",
    "title": "OTESSA25",
    "section": "Two Papers",
    "text": "Two Papers\n\nMadland, C., Irvine, V., DeLuca, C., & Bulut, O. (2024b). Technology-Integrated Assessment: A Literature Review. The Open/Technology in Education, Society, and Scholarship Association Journal, 4(1), 1–48. https://doi.org/10.18357/otessaj.2024.4.1.57\nMadland, C., Irvine, V., DeLuca, C., & Bulut, O. (2024a). Developing the Technology-Integrated Assessment Framework. The Open/Technology in Education, Society, and Scholarship Association Journal, 4(1), 1–19. https://doi.org/10.18357/otessaj.2024.4.1.63"
  },
  {
    "objectID": "posts/otessa25-scale/index.html#components",
    "href": "posts/otessa25-scale/index.html#components",
    "title": "OTESSA25",
    "section": "4 Components",
    "text": "4 Components\n\n\n\n\nAssessment Design\n\n\n\nTechnology Acceptance\n\n\n\nDuty of Care\n\n\n\nPurposes of Assessment"
  },
  {
    "objectID": "posts/otessa25-scale/index.html#section-2",
    "href": "posts/otessa25-scale/index.html#section-2",
    "title": "OTESSA25",
    "section": "",
    "text": "Full Model"
  },
  {
    "objectID": "posts/otessa25-scale/index.html#alt-view",
    "href": "posts/otessa25-scale/index.html#alt-view",
    "title": "OTESSA25",
    "section": "Alt-View",
    "text": "Alt-View\n\n\n\n\n\nflowchart TD\nclassDef purpose fill:#440154,color:#fff,stroke:#440154,stroke-width:4px\nclassDef duty fill:#3B528B,color:#fff,stroke:#3B528B,stroke-width:4px\nclassDef accept fill:#21918c,color:#fff,stroke:#21918c,stroke-width:4px\nclassDef design fill:#5EC962,color:#000,stroke:#5EC962,stroke-width:4px\n  A[Technology-Integrated Assessment] --- B(Assessment Purpose)\n  A(Technology-Integrated Assessment) --- C(Duty of Care)\n  A(Technology-Integrated Assessment) --- D(Technology Acceptance)\n  A(Technology-Integrated Assessment) --- E(Assessment Design)\n  B --- F([Assessment of Learning]) --- G([Assessment for Learning]) --- H([Assessment as Learning])\n  C --- I([Bias]) --- J([Inclusion]) --- K([Relationships]) --- L([Ethical EdTech])\n  D --- M([Performance Expectancy]) --- N([Effort Expectancy]) --- O([Social Influence]) --- P([Facilitating Conditions])\n  E --- Q([Measurement Theory]) --- R([Academic Integrity]) --- S([Relevance]) --- T([Reciprocity])\n  class B,F,G,H purpose\n  class C,I,J,K,L duty\n  class D,M,N,O,P accept\n  class E,Q,R,S,T design"
  },
  {
    "objectID": "posts/otessa25-scale/index.html#differentiation",
    "href": "posts/otessa25-scale/index.html#differentiation",
    "title": "OTESSA25",
    "section": "Differentiation",
    "text": "Differentiation\n\n5Rs of Indigenous Education\n\nRespect\nRelevance\nReciprocity\nResponsibility\nRelationship"
  },
  {
    "objectID": "posts/otessa25-scale/index.html#differentiation-1",
    "href": "posts/otessa25-scale/index.html#differentiation-1",
    "title": "OTESSA25",
    "section": "Differentiation",
    "text": "Differentiation\n\nDuty of Care\n\nBias\nInclusion\nRelationships\nEthical EdTech"
  },
  {
    "objectID": "posts/otessa25-scale/index.html#differentiation-2",
    "href": "posts/otessa25-scale/index.html#differentiation-2",
    "title": "OTESSA25",
    "section": "Differentiation",
    "text": "Differentiation\n\nTechnology Acceptance\n\nPerformance Expectancy\nEffort Expectancy\nSocial Influence\nFacilitating Conditions"
  },
  {
    "objectID": "posts/otessa25-scale/index.html#exploratory-factor-analysis",
    "href": "posts/otessa25-scale/index.html#exploratory-factor-analysis",
    "title": "OTESSA25",
    "section": "Exploratory Factor Analysis",
    "text": "Exploratory Factor Analysis\n\nspecify constructs\nidentify parameters\ngenerate and refine items\ncontent validity"
  },
  {
    "objectID": "posts/otessa25-scale/index.html#funding-and-support",
    "href": "posts/otessa25-scale/index.html#funding-and-support",
    "title": "OTESSA25",
    "section": "Funding and Support",
    "text": "Funding and Support\nThis  research  was supported by the  BCcampus  Research  Fellows  Program, which provides B.C. post-secondary educators and students with funding to conduct small-scale research on teaching and learning, as well as explore evidence-based teaching practices that focus on student success and learning."
  },
  {
    "objectID": "posts/otessa25-scale/index.html#references",
    "href": "posts/otessa25-scale/index.html#references",
    "title": "OTESSA25",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\nSpecial thanks to BCcampus"
  },
  {
    "objectID": "posts/otessa25-specs/index.html",
    "href": "posts/otessa25-specs/index.html",
    "title": "Archives",
    "section": "",
    "text": "Meta-Construct\nConstruct\nDescription\n\n\n\n\nAssessment Purpose\n\n\n\n\n\nAssessment of Learning\nAssessment of learning is what we typically call summative assessment where the intent of the assessment is to certify whether or not learners have achieved the learning outcomes. It occurs at the end of the learning experience and does not have an impact on future teaching or learning. Assessment of learning results in a symbolic representation of each learner’s level of achievement. Symbols, usually letter grades or percentages, are used to communicate with learners, other institutions, regulatory agencies, and other external stakeholders, and are permanent records associated with learners. They are often used to rank learners for the purposes of scholarships and awards, entry into further education, and sometimes for employment purposes.\n\n\n\nAssessment for Learning\nAssessment for learning refers to activities within a learning experience that are designed to provide information on a learner’s current level of achievement. This information is then used to inform future teaching and learning activities. Current levels of achievement are considered to be temporary and can be overridden by future demonstrations of achievement. Both instructors and learners benefit from the ongoing process of opening and closing feedback loops (Carless, 2019), which require feedback to be actively used to improve learning.\n\n\n\nAssessment as Learning\nAssessment as learning is the process of learners developing the skill of “evaluative judgement” (Boud, 2021), which is the ability of learners to judge their own level of achievement in relation to the learning outcomes. It is a reflexive process whereby learners consider the results of their learning strategies and develop more effective strategies to enhance their performance. Assessment as learning allows learners to construct meaning from their experiences, identify gaps in their own knowledge, and to make meaningful decisions about their level of proficiency. Assessment as learning requires learners to be able to discern quality, judge learning processes, manage their biases, assess the trustworthiness of sources and seek opportunities for practice (Boud, 2021).\n\n\nDuty of Care\n\n\n\n\n\nBias\nBias refers to the psychometric term where one group of learners is systematically advantaged or disadvantaged relative to other groups of learners (e.g., Caucasian learners as a group perform better than Asian learners as a group on an assessment due to some characteristic beyond their control) while the ability of each group is the same (Woo et al., 2023). Bias can result from either the inclusion of construct-irrelevant content (e.g., considering learner age when assessing their ability) or the omission of construct-relevant content resulting in variance in scores.\n\n\n\nInclusion\nInclusion involves recognizing marginalized learners as having equitable agency in a learning community (Nieminen, 2024). Marginalization can be based in a wide variety of factors and includes items listed in § 3.1 of the Canadian Human Rights Act (Minister of Justice, RSC 1985).\n\n\n\nRelationships\nRelationship is intended to highlight the importance of designing assessment to foster positive relations between instructors and learners, learners and others in the class, learners and their individual communities (Tessaro et al., 2018), and learners and society as a whole (Nieminen et al., 2022).\n\n\n\nEthical EdTech\nInstructors who use technologies for assessment are obligated to do so in alignment with appropriate codes of ethics. Statements of professional ethics can be related to expectations common to all instructors, such as ensuring that student data is not accessible to third party vendors or advertisers, and they can also be discipline-specific, such as ensuring that physical activity-tracking technology does not disclose learner locations inappropriately (Spector, 2016). Professional ethics involve obligations to individual learners, to society, and to the profession of teaching.\n\n\nTechnology Acceptance\n\n\n\n\n\nPerformance Expectancy\nPerformance expectancy relates to the technology user’s perception that the technology will enable them to perform their job or task at a higher level, it will save them time, or it will reduce the amount of time they need to spend on some tasks (Venkatesh et al., 2003).\n\n\n\nEffort Expectancy\nEffort expectancy relates to the ease of use of the technology. Ease of use is most salient among older women who have less experience with the technology, and it decreases in importance as all users with sustained use (Woo et al., 2023).\n\n\n\nSocial Influence\nSocial influence is the degree to which other people who are important to the user think that they should use the technology. When use of the technology is voluntary, social influence is not a significant factor in predicting acceptance, but when use is mandatory, social influences seem to increase compliance (Woo et al., 2023).\n\n\n\nFacilitating Conditions\nFacilitating conditions are related to the user’s belief that there will be sufficient institutional support in using the technology. When both performance and effort expectancy are high (the technology will improve outcomes with minimal effort), the facilitating conditions construct becomes less significant (Woo et al., 2023)\n\n\nAssessment Design\n\n\n\n\n\nMeasurement\nEducational measurement is a quantitative approach to generating inferences about learner ability, often known as psychomterics. A foundational premiss of psychometrics is that the end result of assessment is always an inference or an interpretation about learner ability based on data collected through various tasks; it is not a direct measurement of a quantity of ability (Pellegrino et al., 2001). In order for inferences of learner ability to be accurate interpretations, they must be valid, reliable, and fair.\n\n\n\nAcademic Integrity\nModern conceptions of academic integrity encompass a comprehensive range of values and behaviours, including: everyday ethics, institutional ethics, ethical leadership, professional and collegial ethics, instructional ethics, student academic conduct, research integrity and ethics, and publication ethics (Eaton, 2024). This description goes beyond ethical learner conduct and extends to the ethical conduct of instructors in the context of instruction and assessment. Dawson (2024) argues that academic integrity is key to instructors being able to derive valid inferences from learner performance.\n\n\n\nReciprocity\nReciprocity refers to the idea that assessment with technology should enrich both learners and instructors, as well as the learners’ communities (Tessaro et al., 2018). Reciprocity must be an intentional component of the design of assessment strategies (Rodriguez-Triana et al., 2020).\n\n\n\nRelevance\nAssessment practice ought to be relevant to the culture of the learner. In Indigenous learning contexts, this suggests an emphasis on oral communication and learning in a community context (Tessaro et al., 2018). We extend this argument and assert that oral communication is a key factor in ensuring ethical practice in any context."
  },
  {
    "objectID": "posts/otessa25-specs/index.html#table-of-specifications",
    "href": "posts/otessa25-specs/index.html#table-of-specifications",
    "title": "Archives",
    "section": "",
    "text": "Meta-Construct\nConstruct\nDescription\n\n\n\n\nAssessment Purpose\n\n\n\n\n\nAssessment of Learning\nAssessment of learning is what we typically call summative assessment where the intent of the assessment is to certify whether or not learners have achieved the learning outcomes. It occurs at the end of the learning experience and does not have an impact on future teaching or learning. Assessment of learning results in a symbolic representation of each learner’s level of achievement. Symbols, usually letter grades or percentages, are used to communicate with learners, other institutions, regulatory agencies, and other external stakeholders, and are permanent records associated with learners. They are often used to rank learners for the purposes of scholarships and awards, entry into further education, and sometimes for employment purposes.\n\n\n\nAssessment for Learning\nAssessment for learning refers to activities within a learning experience that are designed to provide information on a learner’s current level of achievement. This information is then used to inform future teaching and learning activities. Current levels of achievement are considered to be temporary and can be overridden by future demonstrations of achievement. Both instructors and learners benefit from the ongoing process of opening and closing feedback loops (Carless, 2019), which require feedback to be actively used to improve learning.\n\n\n\nAssessment as Learning\nAssessment as learning is the process of learners developing the skill of “evaluative judgement” (Boud, 2021), which is the ability of learners to judge their own level of achievement in relation to the learning outcomes. It is a reflexive process whereby learners consider the results of their learning strategies and develop more effective strategies to enhance their performance. Assessment as learning allows learners to construct meaning from their experiences, identify gaps in their own knowledge, and to make meaningful decisions about their level of proficiency. Assessment as learning requires learners to be able to discern quality, judge learning processes, manage their biases, assess the trustworthiness of sources and seek opportunities for practice (Boud, 2021).\n\n\nDuty of Care\n\n\n\n\n\nBias\nBias refers to the psychometric term where one group of learners is systematically advantaged or disadvantaged relative to other groups of learners (e.g., Caucasian learners as a group perform better than Asian learners as a group on an assessment due to some characteristic beyond their control) while the ability of each group is the same (Woo et al., 2023). Bias can result from either the inclusion of construct-irrelevant content (e.g., considering learner age when assessing their ability) or the omission of construct-relevant content resulting in variance in scores.\n\n\n\nInclusion\nInclusion involves recognizing marginalized learners as having equitable agency in a learning community (Nieminen, 2024). Marginalization can be based in a wide variety of factors and includes items listed in § 3.1 of the Canadian Human Rights Act (Minister of Justice, RSC 1985).\n\n\n\nRelationships\nRelationship is intended to highlight the importance of designing assessment to foster positive relations between instructors and learners, learners and others in the class, learners and their individual communities (Tessaro et al., 2018), and learners and society as a whole (Nieminen et al., 2022).\n\n\n\nEthical EdTech\nInstructors who use technologies for assessment are obligated to do so in alignment with appropriate codes of ethics. Statements of professional ethics can be related to expectations common to all instructors, such as ensuring that student data is not accessible to third party vendors or advertisers, and they can also be discipline-specific, such as ensuring that physical activity-tracking technology does not disclose learner locations inappropriately (Spector, 2016). Professional ethics involve obligations to individual learners, to society, and to the profession of teaching.\n\n\nTechnology Acceptance\n\n\n\n\n\nPerformance Expectancy\nPerformance expectancy relates to the technology user’s perception that the technology will enable them to perform their job or task at a higher level, it will save them time, or it will reduce the amount of time they need to spend on some tasks (Venkatesh et al., 2003).\n\n\n\nEffort Expectancy\nEffort expectancy relates to the ease of use of the technology. Ease of use is most salient among older women who have less experience with the technology, and it decreases in importance as all users with sustained use (Woo et al., 2023).\n\n\n\nSocial Influence\nSocial influence is the degree to which other people who are important to the user think that they should use the technology. When use of the technology is voluntary, social influence is not a significant factor in predicting acceptance, but when use is mandatory, social influences seem to increase compliance (Woo et al., 2023).\n\n\n\nFacilitating Conditions\nFacilitating conditions are related to the user’s belief that there will be sufficient institutional support in using the technology. When both performance and effort expectancy are high (the technology will improve outcomes with minimal effort), the facilitating conditions construct becomes less significant (Woo et al., 2023)\n\n\nAssessment Design\n\n\n\n\n\nMeasurement\nEducational measurement is a quantitative approach to generating inferences about learner ability, often known as psychomterics. A foundational premiss of psychometrics is that the end result of assessment is always an inference or an interpretation about learner ability based on data collected through various tasks; it is not a direct measurement of a quantity of ability (Pellegrino et al., 2001). In order for inferences of learner ability to be accurate interpretations, they must be valid, reliable, and fair.\n\n\n\nAcademic Integrity\nModern conceptions of academic integrity encompass a comprehensive range of values and behaviours, including: everyday ethics, institutional ethics, ethical leadership, professional and collegial ethics, instructional ethics, student academic conduct, research integrity and ethics, and publication ethics (Eaton, 2024). This description goes beyond ethical learner conduct and extends to the ethical conduct of instructors in the context of instruction and assessment. Dawson (2024) argues that academic integrity is key to instructors being able to derive valid inferences from learner performance.\n\n\n\nReciprocity\nReciprocity refers to the idea that assessment with technology should enrich both learners and instructors, as well as the learners’ communities (Tessaro et al., 2018). Reciprocity must be an intentional component of the design of assessment strategies (Rodriguez-Triana et al., 2020).\n\n\n\nRelevance\nAssessment practice ought to be relevant to the culture of the learner. In Indigenous learning contexts, this suggests an emphasis on oral communication and learning in a community context (Tessaro et al., 2018). We extend this argument and assert that oral communication is a key factor in ensuring ethical practice in any context."
  },
  {
    "objectID": "posts/otessa25-specs/index.html#parameters-rough-sample",
    "href": "posts/otessa25-specs/index.html#parameters-rough-sample",
    "title": "Archives",
    "section": "Parameters (Rough Sample)",
    "text": "Parameters (Rough Sample)\n\nAssessment of learning\n- factored into a final grade - occurs after learning - certification of proficiency - communication of proficiency (learners, other institutions, agencies) - expressed symbolically - used to rank learners - (hodge-podge) symbols that conflate achievement and behaviour - quantitative - often uses security measures to prevent academic dishonesty - The teacher is responsible for assessment\n\nPossible questions\nHave you engaged in the following assessment practices:\n\nprovide a grade (numeric or percentage) on all learner work\nban genAI tools to ensure work submitted belongs to each individual learner\nuse tools to detect plagiarism or other forms of cheating\ndeduct ‘points’ for late assignments\nassign grades based on how other learners perform\ncalculate final grades as an average of all assignments throughout the course\n\n\n\n\nAssessment for learning\n\ncreating descriptions to be used to inform future learning\nlearners are evaluated according to their own performance in light of criteria\noccurs multiple times during learning\ndepends on teachers diagnostic skills\nthe teacher is responsible for assessment\nfocus is on improving learning (earl2014?)\n\n\nPossible questions\nHave you engaged in the following assessment practices:\n\nuse learner performance to inform future learning activities\nprovide proficiency feedback related to how learners performed in relation to outcomes (proficiency scale)\nassign ungraded self-check activities throughout the learning\nallow learners to revise and resubmit work that does not meet proficiency standards\ncalculate final grades based on the highest level of proficiency achieved throughout the course\n\n\n\n\nAssessment as Learning\nThe key characteristic of assessment as learning is that learners are challenged to monitor and reflect on their own progress relative to the course outcomes (Boud, 2021). This extends beyond simple self-assessment as it requires learners to engage in multiple complex tasks such as being able to recognize quality in light of stated outcomes, evaluate the quality of their own work, consider the effect of their own biases, determine whether or not a resource or other person is trustworthy, and identify areas in need of practice (Boud et al., 2018). Understanding and being able to recognize quality involves learners developing a clear framework or conceptualization of the domain of knowledge or skill being explored in the unit, course, or program (ajjawi2018?).\n\nlearners monitor their own learning\nlearners make their own adjustments based on their analysis of their proficiency\nLearners use their own knowledge to construct meaning\nlearners identify when they don’t understand something\nthe learner has agency for assessment\n\n\nPossible Questions\nHave you engaged in the following assessment practices:\n\nlearners are provided opportunity to check their understanding to identify gaps in their understanding\nencourage learners to document their learning process to make it visible\nprovide learners opportunity to identify their own questions relevant to the course outcomes\ncollaborate with learners to evaluate the quality of genAI outputs\nask learners to evaluate their own assignments in light of proficiency scales"
  },
  {
    "objectID": "posts/otessa25-specs/index.html#references",
    "href": "posts/otessa25-specs/index.html#references",
    "title": "Archives",
    "section": "References",
    "text": "References\n\n\nBoud, D. (2021). Assessment-as-learning for the development of students’ evaluative judgement. In Z. Yan & L. Yang (Eds.), Assessment as Learning (1st ed., pp. 25–37). Routledge. https://doi.org/10.4324/9781003052081-3\n\n\nBoud, D., Dawson, P., Tai, J., & Ajjawi, R. (2018). Creating an agenda for developing students’ evaluative judgement. In D. Boud, R. Ajjawi, P. Dawson, & J. Tai (Eds.), Developing Evaluative Judgement in Higher Education (1st ed., pp. 186–195). Routledge. https://doi.org/10.4324/9781315109251-20\n\n\nCarless, D. (2019). Feedback loops and the longer-term: Towards feedback spirals. Assessment & Evaluation in Higher Education, 44(5), 705–714. https://doi.org/10.1080/02602938.2018.1531108\n\n\nDawson, P., Bearman, M., Dollinger, M., & Boud, D. (2024). Validity matters more than cheating. Assessment & Evaluation in Higher Education, 1–12. https://doi.org/10.1080/02602938.2024.2386662\n\n\nEaton, S. E. (2024). Comprehensive Academic Integrity (CAI): An Ethical Framework for Educational Contexts. In S. E. Eaton (Ed.), Second Handbook of Academic Integrity (pp. 1–14). Springer Nature Switzerland. https://doi.org/10.1007/978-3-031-54144-5_194\n\n\nMinister of Justice. (RSC 1985). Canadian Human Rights Act, R.S.C.\n\n\nNieminen, J. H. (2024). Assessment for Inclusion: Rethinking inclusive assessment in higher education. Teaching in Higher Education, 29(4), 841–859. https://doi.org/10.1080/13562517.2021.2021395\n\n\nNieminen, J. H., Bearman, M., & Ajjawi, R. (2022). Designing the digital in authentic assessment: Is it fit for purpose? Assessment & Evaluation in Higher Education, 48(0), 1–15. https://doi.org/10.1080/02602938.2022.2089627\n\n\nPellegrino, J. W., Chudowsky, N., & Glaser, R. (2001). Knowing What Students Know: The Science and Design of Educational Assessment. National Academies Press. https://doi.org/10.17226/10019\n\n\nRodriguez-Triana, M. J., Prieto, L. P., Holzer, A., & Gillet, D. (2020). Instruction, Student Engagement, and Learning Outcomes: A Case Study Using Anonymous Social Media in a Face-to-Face Classroom. IEEE Transactions on Learning Technologies, 13(4), 718–733. https://doi.org/10.1109/TLT.2020.2995557\n\n\nSpector, J. M. (2016). Ethics in educational technology: Towards a framework for ethical decision making in and for the discipline. Educational Technology Research and Development, 64(5), 1003–1011. https://doi.org/10.1007/s11423-016-9483-0\n\n\nTessaro, D., Restoule, J.-P., Gaviria, P., Flessa, J., Lindeman, C., & Scully-Stewart, C. (2018). The Five R’s for Indigenizing Online Learning: A Case Study of the First Nations Schools’ Principals Course. Canadian Journal of Native Education, 40(1), 125–143.\n\n\nVenkatesh, V., Morris, M. G., Davis, G. B., & Davis, F. D. (2003). User Acceptance of Information Technology: Toward a Unified View. MIS Quarterly, 27(3), 425–478. https://doi.org/gc8zn2\n\n\nWoo, S. E., LeBreton, J. M., Keith, M. G., & Tay, L. (2023). Bias, Fairness, and Validity in Graduate-School Admissions: A Psychometric Perspective. Perspectives on Psychological Science, 18(1), 3–31. https://doi.org/10.1177/17456916211055374"
  },
  {
    "objectID": "posts/deck-otessa24/index.html#evolving-our-understanding-of-technology-integrated-assessment-a-review-of-the-literature-and-development-of-a-new-framework",
    "href": "posts/deck-otessa24/index.html#evolving-our-understanding-of-technology-integrated-assessment-a-review-of-the-literature-and-development-of-a-new-framework",
    "title": "OTESSA24",
    "section": "Evolving our understanding of technology-integrated assessment: A review of the literature and development of a new framework",
    "text": "Evolving our understanding of technology-integrated assessment: A review of the literature and development of a new framework"
  },
  {
    "objectID": "posts/deck-otessa24/index.html#acknowledgements",
    "href": "posts/deck-otessa24/index.html#acknowledgements",
    "title": "OTESSA24",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nOur team is spread across most of the country and we each live and work on land that has been cared for by Indigenous people for millenia. We each acknowledge that our abundant lives were made possible because of the displacement of the original stewards of the land."
  },
  {
    "objectID": "posts/deck-otessa24/index.html#fall-2019",
    "href": "posts/deck-otessa24/index.html#fall-2019",
    "title": "OTESSA24",
    "section": "Fall 2019",
    "text": "Fall 2019\n2 years into a different dissertation project"
  },
  {
    "objectID": "posts/deck-otessa24/index.html#spring-2020",
    "href": "posts/deck-otessa24/index.html#spring-2020",
    "title": "OTESSA24",
    "section": "Spring 2020",
    "text": "Spring 2020\n\nScreenshot of Ross yelling ‘Pivot’ while trying to move a couch around a tight corner and up some stairs."
  },
  {
    "objectID": "posts/deck-otessa24/index.html#summer-2021",
    "href": "posts/deck-otessa24/index.html#summer-2021",
    "title": "OTESSA24",
    "section": "Summer 2021",
    "text": "Summer 2021\nCandidacy"
  },
  {
    "objectID": "posts/deck-otessa24/index.html#two-papers",
    "href": "posts/deck-otessa24/index.html#two-papers",
    "title": "OTESSA24",
    "section": "Two Papers",
    "text": "Two Papers\nMadland, C., Irvine, V., DeLuca, C., & Bulut, O. (2024b). Technology-Integrated Assessment: A Literature Review. The Open/Technology in Education, Society, and Scholarship Association Journal, 4(1), 1–48. https://doi.org/10.18357/otessaj.2024.4.1.57\nMadland, C., Irvine, V., DeLuca, C., & Bulut, O. (2024a). Developing the Technology-Integrated Assessment Framework. The Open/Technology in Education, Society, and Scholarship Association Journal, 4(1), 1–19. https://doi.org/10.18357/otessaj.2024.4.1.63"
  },
  {
    "objectID": "posts/deck-otessa24/index.html#what-are-your-most-important-concerns-related-to-technology-integrated-assessment",
    "href": "posts/deck-otessa24/index.html#what-are-your-most-important-concerns-related-to-technology-integrated-assessment",
    "title": "OTESSA24",
    "section": "What are your most important concerns related to technology-integrated assessment?",
    "text": "What are your most important concerns related to technology-integrated assessment?\n\n\n\n\nQR Code"
  },
  {
    "objectID": "posts/deck-otessa24/index.html#section",
    "href": "posts/deck-otessa24/index.html#section",
    "title": "OTESSA24",
    "section": "…",
    "text": "…"
  },
  {
    "objectID": "posts/deck-otessa24/index.html#literature-review-madlandtechnologyintegratedassessmentliterature2024",
    "href": "posts/deck-otessa24/index.html#literature-review-madlandtechnologyintegratedassessmentliterature2024",
    "title": "OTESSA24",
    "section": "Literature Review (Madland et al., 2024b)",
    "text": "Literature Review (Madland et al., 2024b)\n\nDesigning assessment in a digital world from Bearman et al. (2022)"
  },
  {
    "objectID": "posts/deck-otessa24/index.html#assessment-design-in-a-digital-world-bearman2022",
    "href": "posts/deck-otessa24/index.html#assessment-design-in-a-digital-world-bearman2022",
    "title": "OTESSA24",
    "section": "Assessment Design in a Digital World (Bearman et al., 2022)",
    "text": "Assessment Design in a Digital World (Bearman et al., 2022)\n\n\nDigital Tools\n\nAssessment Rationales\nLevel of Digital Enhancement (Puentedura, 2009)\nPotential Harms\n\n\nDigital Literacies\n\nMastery or Proficiency\nEvaluation and Critique\n\nHuman Capabilities\n\nFuture Activities\nFuture Self"
  },
  {
    "objectID": "posts/deck-otessa24/index.html#search",
    "href": "posts/deck-otessa24/index.html#search",
    "title": "OTESSA24",
    "section": "Search",
    "text": "Search\n\nPRISMA Diagram of Search Results"
  },
  {
    "objectID": "posts/deck-otessa24/index.html#global-distribution",
    "href": "posts/deck-otessa24/index.html#global-distribution",
    "title": "OTESSA24",
    "section": "Global Distribution",
    "text": "Global Distribution\n\nMap of most common countries of publication."
  },
  {
    "objectID": "posts/deck-otessa24/index.html#predict-the-ranking-of-these-themes",
    "href": "posts/deck-otessa24/index.html#predict-the-ranking-of-these-themes",
    "title": "OTESSA24",
    "section": "Predict the Ranking of these Themes",
    "text": "Predict the Ranking of these Themes\n\nQR Code"
  },
  {
    "objectID": "posts/deck-otessa24/index.html#section-1",
    "href": "posts/deck-otessa24/index.html#section-1",
    "title": "OTESSA24",
    "section": "…",
    "text": "…"
  },
  {
    "objectID": "posts/deck-otessa24/index.html#actual-ranking",
    "href": "posts/deck-otessa24/index.html#actual-ranking",
    "title": "OTESSA24",
    "section": "Actual Ranking",
    "text": "Actual Ranking\n\nNumber of Citations by Theme in the Literature."
  },
  {
    "objectID": "posts/deck-otessa24/index.html#comparing-themes-to-bearman2022",
    "href": "posts/deck-otessa24/index.html#comparing-themes-to-bearman2022",
    "title": "OTESSA24",
    "section": "Comparing Themes to Bearman et al. (2022)",
    "text": "Comparing Themes to Bearman et al. (2022)\n\nColours show different levels of alignment between Bearman et al. (2022) and the themes in the literature."
  },
  {
    "objectID": "posts/deck-otessa24/index.html#what-do-you-notice-what-do-you-wonder",
    "href": "posts/deck-otessa24/index.html#what-do-you-notice-what-do-you-wonder",
    "title": "OTESSA24",
    "section": "What do you notice? What do you wonder?",
    "text": "What do you notice? What do you wonder?\n\nQR Code"
  },
  {
    "objectID": "posts/deck-otessa24/index.html#section-2",
    "href": "posts/deck-otessa24/index.html#section-2",
    "title": "OTESSA24",
    "section": "…",
    "text": "…"
  },
  {
    "objectID": "posts/deck-otessa24/index.html#missing-from-the-literature",
    "href": "posts/deck-otessa24/index.html#missing-from-the-literature",
    "title": "OTESSA24",
    "section": "Missing from the Literature",
    "text": "Missing from the Literature\n\nLevel of Digital Enhancement (SAMR)\nEvaluation and Critique of Digital Tools (Digital Literacies)\nHuman Capabilities\n\nDropped"
  },
  {
    "objectID": "posts/deck-otessa24/index.html#missing-from-the-bearman2022-framework",
    "href": "posts/deck-otessa24/index.html#missing-from-the-bearman2022-framework",
    "title": "OTESSA24",
    "section": "Missing from the Bearman et al. (2022) Framework",
    "text": "Missing from the Bearman et al. (2022) Framework\n\nEfficiency and Workload\nAcademic Integrity\nCommunity"
  },
  {
    "objectID": "posts/deck-otessa24/index.html#missing-from-both-literature-and-framework",
    "href": "posts/deck-otessa24/index.html#missing-from-both-literature-and-framework",
    "title": "OTESSA24",
    "section": "Missing from both Literature and Framework",
    "text": "Missing from both Literature and Framework\n\nIndigenous approaches\n\nRespect\nRelevance\nReciprocity\nResponsibility\nRelationship"
  },
  {
    "objectID": "posts/deck-otessa24/index.html#maintaining",
    "href": "posts/deck-otessa24/index.html#maintaining",
    "title": "OTESSA24",
    "section": "Maintaining",
    "text": "Maintaining\nPurposes of Assessment\n\n…OF Learning\n…FOR Learning\n…AS Learning"
  },
  {
    "objectID": "posts/deck-otessa24/index.html#modifying",
    "href": "posts/deck-otessa24/index.html#modifying",
    "title": "OTESSA24",
    "section": "Modifying",
    "text": "Modifying\n\nAssessment Design"
  },
  {
    "objectID": "posts/deck-otessa24/index.html#consolidating",
    "href": "posts/deck-otessa24/index.html#consolidating",
    "title": "OTESSA24",
    "section": "Consolidating",
    "text": "Consolidating\n\nWorkload + Efficiency = Technology Acceptance\nHuman Capabilities + Potential Harms = Duty of Care"
  },
  {
    "objectID": "posts/deck-otessa24/index.html#utaut",
    "href": "posts/deck-otessa24/index.html#utaut",
    "title": "OTESSA24",
    "section": "UTAUT",
    "text": "UTAUT\n\nSimplified Diagram of the Unified Theory of Acceptance and Use of Technology"
  },
  {
    "objectID": "posts/deck-otessa24/index.html#extending",
    "href": "posts/deck-otessa24/index.html#extending",
    "title": "OTESSA24",
    "section": "Extending",
    "text": "Extending\n\nDeLuca et al. (2016) - Measurement Theory"
  },
  {
    "objectID": "posts/deck-otessa24/index.html#indigenizing",
    "href": "posts/deck-otessa24/index.html#indigenizing",
    "title": "OTESSA24",
    "section": "Indigenizing",
    "text": "Indigenizing\n\nCommunity/Relationships & Respect & Responsibility –&gt; Duty of Care\nRelevance & Reciprocity –&gt; Assessment Design"
  },
  {
    "objectID": "posts/deck-otessa24/index.html#components",
    "href": "posts/deck-otessa24/index.html#components",
    "title": "OTESSA24",
    "section": "4 Components",
    "text": "4 Components\n\n\n\n\nAssessment Design\n\n\n\nTechnology Acceptance\n\n\n\nDuty of Care\n\n\n\nPurposes of Assessment"
  },
  {
    "objectID": "posts/deck-otessa24/index.html#section-5",
    "href": "posts/deck-otessa24/index.html#section-5",
    "title": "OTESSA24",
    "section": "",
    "text": "Full Model"
  },
  {
    "objectID": "posts/deck-otessa24/index.html#alt-view",
    "href": "posts/deck-otessa24/index.html#alt-view",
    "title": "OTESSA24",
    "section": "Alt-View",
    "text": "Alt-View\n\n\n\n\n\nflowchart TD\nclassDef purpose fill:#440154,color:#fff,stroke:#440154,stroke-width:4px\nclassDef duty fill:#3B528B,color:#fff,stroke:#3B528B,stroke-width:4px\nclassDef accept fill:#21918c,color:#fff,stroke:#21918c,stroke-width:4px\nclassDef design fill:#5EC962,color:#000,stroke:#5EC962,stroke-width:4px\n  A[Technology-Integrated Assessment] --- B(Assessment Purpose)\n  A(Technology-Integrated Assessment) --- C(Duty of Care)\n  A(Technology-Integrated Assessment) --- D(Technology Acceptance)\n  A(Technology-Integrated Assessment) --- E(Assessment Design)\n  B --- F([Assessment of Learning]) --- G([Assessment for Learning]) --- H([Assessment as Learning])\n  C --- I([Bias]) --- J([Inclusion]) --- K([Relationships]) --- L([Ethical EdTech])\n  D --- M([Performance Expectancy]) --- N([Effort Expectancy]) --- O([Social Influence]) --- P([Facilitating Conditions])\n  E --- Q([Measurement Theory]) --- R([Academic Integrity]) --- S([Relevance]) --- T([Reciprocity])\n  class B,F,G,H purpose\n  class C,I,J,K,L duty\n  class D,M,N,O,P accept\n  class E,Q,R,S,T design"
  },
  {
    "objectID": "posts/deck-otessa24/index.html#funding-and-support",
    "href": "posts/deck-otessa24/index.html#funding-and-support",
    "title": "OTESSA24",
    "section": "Funding and Support",
    "text": "Funding and Support\nThis  research  was supported by the  BCcampus  Research  Fellows  Program, which provides B.C. post-secondary educators and students with funding to conduct small-scale research on teaching and learning, as well as explore evidence-based teaching practices that focus on student success and learning."
  },
  {
    "objectID": "posts/deck-otessa24/index.html#references",
    "href": "posts/deck-otessa24/index.html#references",
    "title": "OTESSA24",
    "section": "References",
    "text": "References\n\n\nBearman, M., Nieminen, J., & Ajjawi, R. (2022). Designing assessment in a digital world: An organising framework. Assessment & Evaluation in Higher Education, 48(3). https://doi.org/10.1080/02602938.2022.2069674\n\n\nDeLuca, C., LaPointe-McEwan, D., & Luhanga, U. (2016). Approaches to classroom assessment inventory: A new instrument to support teacher assessment literacy. Educational Assessment, 21, 248–266. https://doi.org/gfgtsg\n\n\nMadland, C., Irvine, V., DeLuca, C., & Bulut, O. (2024a). Developing the Technology-Integrated Assessment Framework. The Open/Technology in Education, Society, and Scholarship Association Journal, 4(1), 1–19. https://doi.org/10.18357/otessaj.2024.4.1.63\n\n\nMadland, C., Irvine, V., DeLuca, C., & Bulut, O. (2024b). Technology-Integrated Assessment: A Literature Review. The Open/Technology in Education, Society, and Scholarship Association Journal, 4(1), 1–48.\n\n\nPuentedura, R. R. (2009). Building Transformation: An Introduction to the SAMR Model. In http://hippasus.com.\n\n\n\n\n\n\n\nSpecial thanks to BCcampus"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "slides",
    "section": "",
    "text": "Archives\n\n\n\n\n\n\notessa25\n\n\n\n\n\n\n\n\n\nJun 5, 2025\n\n\nColin Madland\n\n\n\n\n\n\n\n\n\n\n\n\nOTESSA25\n\n\n\n\n\n\n\n\n\n\n\nJun 5, 2025\n\n\nColin Madland1, Valerie Irvine1, Christopher DeLuca2, and Okan Bulut3\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nMay 17, 2025\n\n\nTristan O’Malley\n\n\n\n\n\n\n\n\n\n\n\n\nOTESSA24\n\n\n\n\n\n\n\n\n\n\n\nJun 20, 2024\n\n\nColin Madland1, Valerie Irvine1, Christopher DeLuca2, and Okan Bulut3\n\n\n\n\n\n\nNo matching items"
  }
]